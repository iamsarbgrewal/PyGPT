{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing & importing libraries"
      ],
      "metadata": {
        "id": "5fRc75ZC1zTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "3Xad7ucaKoiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24QYIlUwKaTo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, TextDataset, EvalPrediction\n",
        "from scipy.special import softmax\n",
        "from sklearn.metrics import log_loss\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting drive to load dataset from and save the model to!"
      ],
      "metadata": {
        "id": "9Siec1g816pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "mfE9cIxAKzSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining `compute_metrics` to calculate perplexity"
      ],
      "metadata": {
        "id": "5I4dyyZe2EsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "    logits = p.predictions\n",
        "    labels = p.label_ids\n",
        "    probabilities = softmax(logits, axis=-1)\n",
        "    loss = log_loss(labels.flatten(), probabilities.reshape(-1, probabilities.shape[-1]), labels=[i for i in range(logits.shape[-1])])\n",
        "    perplexity = np.exp(loss)\n",
        "    return {\"perplexity\": perplexity}"
      ],
      "metadata": {
        "id": "X2BLq6bUkiIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traning the model with Huggingface's `Trainer` class"
      ],
      "metadata": {
        "id": "gPtSe3Mu2MrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_chatbot():\n",
        "    model_output_path = '/content/drive/MyDrive/data/models'\n",
        "\n",
        "    # Set up the tokenizer and model\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
        "    # model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Prepare the dataset\n",
        "    train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"/content/drive/MyDrive/data/training.txt\", block_size=128)\n",
        "    val_dataset = TextDataset(tokenizer=tokenizer, file_path=\"/content/drive/MyDrive/data/validation.txt\", block_size=128)\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    # Set up the training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=model_output_path,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=4,\n",
        "        save_strategy='steps',\n",
        "        save_steps=100,\n",
        "        evaluation_strategy='steps',\n",
        "        eval_steps=100,\n",
        "        save_total_limit=2,\n",
        "        report_to='none',\n",
        "        prediction_loss_only=True,\n",
        "        learning_rate = 5e-5\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(model_output_path)\n",
        "\n",
        "    # Save the tokenizer\n",
        "    tokenizer.save_pretrained(model_output_path)\n"
      ],
      "metadata": {
        "id": "rlr3PyVaLljx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_chatbot()"
      ],
      "metadata": {
        "id": "0WEDqR5cMDsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating response"
      ],
      "metadata": {
        "id": "lMI6_l9A2TBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('/content/drive/MyDrive/data/models/gpt2-large-ft')\n",
        "    model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/data/models/gpt2-large-ft/checkpoint-400')\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Create the attention mask and pad token id\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=256,\n",
        "        attention_mask=attention_mask,\n",
        "        num_return_sequences=1, # Generate a single sequence\n",
        "        temperature=0.85,       # Controls randomness (higher for more diversity)\n",
        "        repetition_penalty=1.2, # Controls repetition (higher for less repetition)\n",
        "        pad_token_id=tokenizer.eos_token_id,  # ID of the padding token (\"<EOS>\")\n",
        "        early_stopping=True,\n",
        "        top_k = 50\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "vkjfn8feZueS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}